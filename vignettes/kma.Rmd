---
title: "Computing Intron Retention With Keep Me Around (`kma`)"
author: "Harold Pimentel"
date: "`r Sys.Date()`"
output:
  rmarkdown::html_document:
    toc: true
vignette: >
  %\VignetteIndexEntry{Walkthrough}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

This vignette described how to run the Keep Me Around `kma` suite to compute
intron retention in RNA-Seq experiments.

# General pipeline

The general pipeline is as follows:

1. **Pre-process** to generate intron coordinates and sequences
1. **Quantify** transcript expression using
   [eXpress](http://bio.math.berkeley.edu/eXpress/) against augmented
   transcriptome containing introns
1. **Post-process** to compute intron retention using `R` package

**TODO: Put a more detailed flowchart here**

# Installing

There are two required portions of the tool to perform intron retention
quantification. The **pre-process** step is written in python

## Installing the pre-processing tools

If you have installed the `R` package (see installing the post-processing
tools), then you have successfully installed the
pre-processing tools. From `R`, you can find the path by typing:

```{r}
system.file("pre-process", package="kma")
```

The only additional dependencies are the Python packages `pyfasta`, `biopython` and `pysam`.
Both packages can be installed via PyPI:

```{bash}
pip install pyfasta biopython pysam
```

## Installing the quantification tools

The tools needed for quantification are:

- [Bowtie 2](http://bowtie-bio.sourceforge.net/bowtie2/) short read aligner
- [eXpress](http://bio.math.berkeley.edu/eXpress/) RNA-Seq quantification tool

Please see the corresponding documentation on their respective web pages.

## Installing the post-processing tools

The post-processing tools are contained in an `R` packaged called `kma`.
Currently, the latest package is on
[Github](http://github.com/pachterlab/kma). To install, make sure you have
the `devtools` package installed and type

```{r, eval=FALSE}
devtools::install_github("http://github.com/pachterlab/kma")
```

# Generate intron coordinates (pre-processing)

The path can be found in R by typing `system.file("preprocess",
package="kma")`. Open a terminal and put this path in an environment variable:

```{bash}
PRE=/path/to/kma/preprocess
```

If you are on a Mac, the path might look like: /Library/Frameworks/R.framework/Versions/3.1/Resources/library/kma/preprocess

We are now ready to generate the introns. A typical call looks as follows:

```{bash}
python $PRE/generate_introns.py --genome seq.fa --gtf trans.gtf --extend N --out out_dir
```

With the inputs being:

- `--genome seq.fa`: **genome sequence**  in [Multi-FASTA
  format](http://en.wikipedia.org/wiki/FASTA_format) where contig names
  correspond to the GTF file.
- `--gtf trans.gtf`: **annotation file** in [GTF
  format](http://www.ensembl.org/info/website/upload/gff.html).
- `--extend N`: Optional. If set, `N` is the number of bases bases to overlap
  into the intron. Note, this should be at most `read_length - 1`.
- `--out out_dir`: A directory to write the outputs. Will be created if doesn't
  already exist.

The following outputs will then be put in `out_dir`:

- **introns.fa** - a FASTA file containing the intron sequences.
- **introns.bed** - BED file with coordinates used to quantify intron.
- **intron\_to\_transcripts.txt** - used later.

# Quantification

**Note**: Technically any quantification tool can be used with `kma_ir`, but currently
only support with `eXpress` is implemented. Please contact me if you're
interested in a quantification tool being supported.

Here, we will discuss quantification. After `generate_introns.py` is run,
`introns.fa` should be combined with the full transcript sequences. This can be
done using the Linux command `cat`:

```{bash}
cat trans.fa introns.fa > trans_and_introns.fa
```

We assume the file name is `trans_and_introns.fa` in the following sections, but
the file name can be anything.

After you've done this, this section requires the following steps:

1. Create the `Bowtie 2` index
1. Align reads to the augmented transcriptome
1. Quantify against the augmented transcriptome

## Creating the Bowtie 2 index

See the `Bowtie 2` manual for more advanced options.

```{bash}
bowtie2-build --offrate 1 trans_and_introns.fa trans_and_introns
```

This only has to be run once if you decide to change the gene annotation.

## Align reads

Once you have a `Bowtie 2` index, you can align any number of RNA-Seq
experiments to that index. The following arguments are recommended when running
Bowtie 2:

```{bash}
bowtie2 -k 200 --rdg 6,5 --rfg 6,5 --score-min L,-.6,-.4 -X trans_and_introns
    -1 left.fastq -2 right.fastq | samtools view -Sb - > hits.bam
```

## Quantify

Once you've aligned the reads, you can run eXpress against the alignments. See
the eXpress website for additional arguments. The general eXpress call is as
follows:

```{bash}
express trans_and_introns.fa hits.bam
```

# Computing intron retention (post-processing)

The first step is to the load the quantification data into `R`. The function
`read_express` takes a list of file names, sample names, and condition names.
`read_express` then returns a list with the attributes:

- `tpm` - a `data.frame` with TPM of all samples
- `uniq_counts` - a `data.frame` with the number of unique counts of all
  samples
- `all_data` - a list of `data.frame`s from all the eXpress output. Sorted by
  `target_id`.
- `sample` - a character vector for each sample, describing the sample (e.g.
  `tumor_rep1`)
- `condition` - a character vector for each sample describing the grouping
  (e.g. `tumor`)

# A worked example

We will run through the example **TODO** put link here:

## Organization
We recommend organizing your experiments in the following format:

```
experiment
|____conditionA
| |____rep1
| |____rep2
| |____rep3
|____conditionB
| |____rep1
| |____rep2
| |____rep3
```

where `experiment` is the top level for the particular set of experiments,
`conditionX` refers to the condition (e.g. tumor, or control), and `repY`
represents the biological replicates. This allows for a structured way to keep
track of your data as well as an easy way to load it in `R`.

## Pre-processing

Assuming you downloaded the data to __...__

Run `system.file` in `R` to get the pre-processing directory (see the
pre-processing section above for more information):

```{r, eval=FALSE}
system.file("pre-process", package = "kma")

```

Then, store this value in an environment variable in your shell:

```
PRE=/path/to/kma/pre-process
```

Let's move to the directory and run the pre-processing:

```
cd example
python $PRE/generate_introns.py --genome genome/chr2.fa \
    --gtf annotation/refGene_sf3b1.gtf --extend 25 \
    --out kma_pre-process_out
```

The output files will then be stored in `kma_pre-process_out` which you can
view using `ls kma_pre-process_out`:

```
intron_to_transcripts.txt introns.bed               introns.fa
```

The `introns.fa` file can now be merged with the transcriptome sequences in the
quantification section.

## Quantification

### Merging the transcriptome and intron sequences

Next, we need to merge the transcriptome and intron sequences. This can be done
using the command `cat`:

```
cat annotation/sf3b1.fa kma_pre-process_out/introns.fa > annotation/sf3b1_with_introns.fa
```

The file `annotations/sf3b1_with_introns.fa` now contains the transcriptome
sequences and the intron sequences. This FASTA file can now be used to build
a `Bowtie2` index for alignment.

### Building the `Bowtie2` index

We want to align against the introns and transcripts. Thus, we will build
a `Bowtie2` index. See the `Bowtie2` website for more information on the
parameters:

```{bash}
bowtie2-build --offrate 1 annotation/sf3b1_with_introns.fa annotation/sf3b1_with_introns
```

This call will result in many Bowtie index files (`*.bt2` files).

### Alignment

For alignment, we will only align one sample as aligning other samples can be
done similarly and can be automated. Please note, the reads we are aligning are
single-end -- the command for paired-end reads can be found on the Bowtie
2 website.

```{bash}
bowtie2 -k 200 --rdg 6,5 --rfg 6,5 --score-min L,-.6,-.4 -x annotation/sf3b1_with_introns \
    -U experiment/ortho/ortho1/*.fastq.gz |
    samtools view -Sb - > experiment/ortho/ortho1/sf3b1.bam
```

This will result in a BAM file `experiment/ortho/ortho1/sf3b1.bam` which can
then be quantified using eXpress.

### Quantification with eXpress

Now that we have alignment files, we can quantify isoform and intron expression
with eXpress:

```{bash}
express -o experiment/ortho/ortho1/xprs_out annotation/sf3b1_with_introns.fa \
    experiment/ortho/ortho1/sf3b1.bam
```

The quantification results will now be in `experiment/ortho/ortho1/xprs_out`.
The other samples can be processed similarly.

## Post-processing

### Loading the files

Since we organized our data nicely, we can load the data into R quite easily
using `Sys.glob`:

```{r}
base_dir <- system.file("example", package="kma")
xprs_fnames <- Sys.glob(file.path(base_dir, "experiment/*/*/xprs_out/results.xprs"))
xprs_fnames
```

Sample names can be inferred from the organization as well:

```{r}
sample_names <- sub(file.path(base_dir, "experiment/[a-z]+/"), "", xprs_fnames) %>%
    sub("xprs_out/results.xprs", "", .) %>%
    gsub("/", "", .)
sample_names
```

Assuming you label your replicates according to the condition they are a part
of, you can also infer condition names:

```{r}
condition_names <- sub("[0-9]+", "", sample_names)
condition_names
```

You now have:

- `xprs_fnames` - a character vector of file names pointing to the
  quantification files
- `sample_names` - a character vector of identifiers for each sample
- `condition_names` - a character vector of identifiers of conditions

We can now load all the eXpress data into R:

```{r}
xprs <- read_express(xprs_fnames, sample_names, condition_names)
```

This results in a named list with the following members:

```{r}
names(xprs)
```

We only need one other file, `intron_to_transcripts.txt`, which was created
during the pre-processing step. We can read that in using `read.table` or
`data.table::fread` which is much faster, just be sure to use the `data.table
= FALSE` flag as seen below:

```{r}
intron_to_trans <- data.table::fread(file.path(base_dir, "kma_pre-process_out",
    "intron_to_transcripts.txt"), data.table = FALSE)
head(intron_to_trans)
```

To create an `IntronRetention` object, you can call `newIntronRetention` with
the following command:

```{r}
ir <- newIntronRetention(xprs$tpm, intron_to_trans, xprs$condition,
    xprs$uniq_counts)
```

Printing the `IntronRetention` object results in a short summary:

```{r}
print(ir)
```


### Filtering

Filter functions have the following format: `filter_name(ir, options)` where
`ir` is an `IntronRetention` object, and options are options for the filter.
They return a `IntronRetention` object with an updated `flat` member. Filters
can be implemented by the user, but should return `TRUE` if the intron passes
the filter and `FALSE` otherwise. The column name for each filter should begin
with `f_`.

This example filters out denominators that are below 1 TPM, introns that have
exactly 0 or 1 PSI (due to either no contributions from the intron coverage or
solely coverage from the intron). `filter_low_frags` filters out introns that
don't have `N` number of unique reads:

```{r}
ir <- ir %>%
    filter_low_tpm(1) %>%
    filter_perfect_psi() %>%
    filter_low_frags(3)
colnames(ir$flat)
```

#### Zero coverage

Zero coverage filter has to be computed on the read alignments and
corresponding eXpress results.

```{bash}
python $PRE/zeroCoverage.py experiment/ortho/ortho1/xprs_out/results.xprs \
    experiment/ortho/ortho1/sf3b1.bam \
    experiment/ortho/ortho1/zero_coverage.txt
```

The output is then in `experiment/ortho/ortho1/zero_coverage.txt`. Zero
coverage data can be read in using the function `get_batch_intron_zc`. Like
`read_express` it needs file names, sample names, and condition names:

```{r}
zc_fnames <- Sys.glob(file.path(base_dir, "experiment/*/*/zero_coverage.txt"))
zc_samples <- sub(file.path(base_dir, "experiment/[a-z]+/"), "", zc_fnames) %>%
    sub("zero_coverage.txt", "", .) %>%
    gsub("/", "", .)
zc_conditions <- sub("[0-9]+", "", zc_samples)
all_zc <- get_batch_intron_zc(zc_fnames, zc_samples, zc_conditions)
head(all_zc)
```

The result from `get_batch_intron_zc` is a `data.frame` that can be inspected
manually if you'd like. To incorporate it to `kma`, use
`summarize_zero_coverage`:

```{r}
ir <- summarize_zero_coverage(ir, all_zc)
```

This adds a new column to `ir$flat` called `f_zc_*`:

```{r}
colnames(ir$flat)
```

### Hypothesis testing

Hypothesis testing first aggregates all filters (by taking the intersection of
all filters), computes the null distribution, then returns a data frame
summarizing the test:

```{r}
ir_test <- retention_test(ir)
head(ir_test)
```

Significant introns can be found using the dplyr function `filter`:

```{r}
ir_test %>%
    filter(qvalue <= 0.10) %>%
    select(-c(pvalue))
```

In this case we only see one significant intron. Since we have such a small
dataset, the null distribution isn't that reliable so the results shouldn't be
taken too seriously, but when the data set is larger (like in a normal
experiment), the results are more reliable.
